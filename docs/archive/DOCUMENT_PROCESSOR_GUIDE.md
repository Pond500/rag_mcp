# ğŸ“š Document Processor Architecture Guide

## ğŸ¯ Overview

à¸£à¸°à¸šà¸š Document Processing à¹à¸šà¸š **3-Tier Progressive Extraction** à¸—à¸µà¹ˆà¸­à¸­à¸à¹à¸šà¸šà¸¡à¸²à¹€à¸à¸·à¹ˆà¸­:
- âœ… **à¸›à¸£à¸°à¸«à¸¢à¸±à¸”à¸„à¹ˆà¸²à¹ƒà¸Šà¹‰à¸ˆà¹ˆà¸²à¸¢** - à¹€à¸£à¸´à¹ˆà¸¡à¸ˆà¸²à¸ tier à¸–à¸¹à¸à¸—à¸µà¹ˆà¸ªà¸¸à¸”à¸à¹ˆà¸­à¸™
- âœ… **à¸„à¸¸à¸“à¸ à¸²à¸à¸ªà¸¹à¸‡** - Escalate à¹„à¸› tier à¸—à¸µà¹ˆà¸”à¸µà¸à¸§à¹ˆà¸²à¸–à¹‰à¸²à¸„à¸¸à¸“à¸ à¸²à¸à¹„à¸¡à¹ˆà¸à¸­
- âœ… **à¸£à¸­à¸‡à¸£à¸±à¸šà¸«à¸¥à¸²à¸¢ format** - PDF, Word, Excel, PowerPoint
- âœ… **à¹„à¸¡à¹ˆà¸¡à¸µ Tesseract OCR** - à¸«à¸¥à¸µà¸à¹€à¸¥à¸µà¹ˆà¸¢à¸‡ "OSD failed" errors

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    User Upload PDF                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Tier 1: FAST (Docling - NO OCR)                            â”‚
â”‚  â€¢ Cost: $0.00 (FREE)                                       â”‚
â”‚  â€¢ Time: ~5-10 seconds                                      â”‚
â”‚  â€¢ Quality Target: â‰¥ 0.70                                   â”‚
â”‚  â€¢ Works on: PDFs with text layer                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â”‚ Quality < 0.70 or Empty?
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Tier 2: BALANCED (Gemini Flash Free VLM)                   â”‚
â”‚  â€¢ Cost: $0.00 (FREE)                                       â”‚
â”‚  â€¢ Time: ~30-60 seconds                                     â”‚
â”‚  â€¢ Quality Target: â‰¥ 0.80                                   â”‚
â”‚  â€¢ Works on: Image-based PDFs, scanned documents            â”‚
â”‚  â€¢ Limit: 1500 requests/day                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â”‚ Quality < 0.80 or Rate Limited?
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Tier 3: PREMIUM (Gemini Pro / Claude 3.5)                  â”‚
â”‚  â€¢ Cost: $0.0013-0.003/page                                 â”‚
â”‚  â€¢ Time: ~60 seconds/page                                   â”‚
â”‚  â€¢ Quality Target: â‰¥ 0.95                                   â”‚
â”‚  â€¢ Works on: Complex documents, Thai language               â”‚
â”‚  â€¢ No limit (paid tier)                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“‚ File Structure

```
src/
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ document_processor.py         # ğŸ”§ Core extraction (Docling)
â”‚   â”œâ”€â”€ progressive_processor.py      # ğŸ¯ 3-tier orchestrator
â”‚   â”œâ”€â”€ openrouter_extractor.py       # ğŸ¤– VLM wrapper (OpenRouter)
â”‚   â”œâ”€â”€ quality_checker.py            # ğŸ“Š Quality scoring (5 dimensions)
â”‚   â””â”€â”€ router.py                     # ğŸ§­ Document type routing
â”‚
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ text_cleaner.py               # ğŸ§¹ Text cleaning & artifacts removal
â”‚   â””â”€â”€ document_validator.py         # âœ… Validation & reporting
â”‚
â””â”€â”€ models/
    â”œâ”€â”€ embeddings.py                 # ğŸ”¢ Text embeddings
    â””â”€â”€ reranker.py                   # ğŸ¯ Search result reranking
```

---

## ğŸ”§ Core Components

### 1. **DocumentProcessor** (`document_processor.py`)

**à¸«à¸™à¹‰à¸²à¸—à¸µà¹ˆ:** Extract à¸‚à¹‰à¸­à¸„à¸§à¸²à¸¡à¸ˆà¸²à¸ documents à¸”à¹‰à¸§à¸¢ Docling (à¹„à¸¡à¹ˆà¹ƒà¸Šà¹‰ Tesseract OCR)

**Key Features:**
- âœ… à¸£à¸­à¸‡à¸£à¸±à¸š PDF, DOCX, XLSX, PPTX, TXT, MD
- âœ… Structured section extraction (à¸£à¸±à¸à¸©à¸²à¹‚à¸„à¸£à¸‡à¸ªà¸£à¹‰à¸²à¸‡ headers/tables)
- âœ… Fallback to full markdown à¸–à¹‰à¸² structured à¹„à¸¡à¹ˆà¸ªà¸³à¹€à¸£à¹‡à¸ˆ
- âœ… Thai encoding fix (à¹à¸à¹‰à¸›à¸±à¸à¸«à¸² à¸”ï¿½à¸² â†’ à¸”à¸³)
- âœ… HTML comments & base64 images removal

**Configuration:**
```python
DocumentProcessor(
    enable_ocr=False,           # âŒ à¹„à¸¡à¹ˆà¹ƒà¸Šà¹‰ Tesseract!
    table_mode="accurate",      # à¹ƒà¸Šà¹‰ TableFormer ACCURATE mode
    config=settings
)
```

**Extraction Flow:**
```python
def _extract_with_docling(file_path):
    # 1. Convert PDF with Docling
    result = converter.convert(file_path)
    
    # 2. Try structured extraction
    sections = _extract_structured_sections(result.document)
    
    # 3. Check if extraction is valid
    has_base64_images = any('data:image/' in s for s in sections)
    has_only_comments = all('<!--' in s for s in sections)
    
    # 4. Fallback to full markdown if needed
    if has_base64_images or has_only_comments or len(sections) < 3:
        full_markdown = result.document.export_to_markdown()
        sections = _split_by_size(full_markdown)
    
    # 5. Clean artifacts
    sections = [_clean_markdown(s) for s in sections]
    
    return sections
```

**Fallback Conditions:**
```python
should_fallback = (
    not sections                                      # à¹„à¸¡à¹ˆà¸¡à¸µ sections
    or all(len(s.strip()) < 50 for s in sections)   # sections à¸ªà¸±à¹‰à¸™à¹€à¸à¸´à¸™à¹„à¸›
    or (len(sections) < 3 and avg_chars < 500)       # sections à¸™à¹‰à¸­à¸¢ + à¸ªà¸±à¹‰à¸™
    or has_only_comments                              # à¸¡à¸µà¹à¸•à¹ˆ HTML comments
    or has_base64_images                              # à¸¡à¸µ base64 encoded images
)
```

---

### 2. **ProgressiveDocumentProcessor** (`progressive_processor.py`)

**à¸«à¸™à¹‰à¸²à¸—à¸µà¹ˆ:** Orchestrate 3-tier extraction strategy

**Tier Configuration:**
```python
TIER_THRESHOLDS = {
    'fast': 0.70,      # Docling à¸•à¹‰à¸­à¸‡à¹„à¸”à¹‰à¸­à¸¢à¹ˆà¸²à¸‡à¸™à¹‰à¸­à¸¢ 0.70
    'balanced': 0.80,  # VLM Free à¸•à¹‰à¸­à¸‡à¹„à¸”à¹‰à¸­à¸¢à¹ˆà¸²à¸‡à¸™à¹‰à¸­à¸¢ 0.80
    'premium': 0.85    # VLM Premium à¸•à¹‰à¸­à¸‡à¹„à¸”à¹‰à¸­à¸¢à¹ˆà¸²à¸‡à¸™à¹‰à¸­à¸¢ 0.85
}

TIER_COSTS = {
    'fast': 0.0,        # Docling - FREE
    'balanced': 0.0,    # Gemini Flash Free - FREE (but rate limited)
    'premium': 0.0013   # Gemini Pro - ~$0.0013/page
}
```

**Extraction Logic:**
```python
def extract_with_smart_routing(pdf_path, target_quality=0.70):
    best_result = None
    best_quality = 0.0
    
    for tier in ['fast', 'balanced', 'premium']:
        # Extract with current tier
        pages, quality_report = _extract_tier(tier, pdf_path)
        
        # Track best result
        if quality_report.overall_score > best_quality:
            best_result = (pages, quality_report, tier)
            best_quality = quality_report.overall_score
        
        # Stop if target met
        if quality_report.overall_score >= target_quality:
            break
    
    return ExtractionResult(**best_result)
```

**Tier Extraction:**
```python
def _extract_tier(tier, pdf_path):
    if tier == 'fast':
        # Use Docling (no OCR)
        pages = fast_processor.extract_text(pdf_path)
        
    elif tier in ['balanced', 'premium']:
        # Use VLM (OpenRouter)
        model = 'free' if tier == 'balanced' else 'premium'
        extractor = OpenRouterExtractor(api_key, model=model)
        pages = extractor.extract_from_pdf(pdf_path, dpi=image_dpi)
    
    # Check quality
    quality_report = quality_checker.check_quality(pages)
    
    return pages, quality_report
```

---

### 3. **OpenRouterExtractor** (`openrouter_extractor.py`)

**à¸«à¸™à¹‰à¸²à¸—à¸µà¹ˆ:** VLM extraction wrapper for OpenRouter API

**Supported Models:**
```python
MODELS = {
    'free': 'google/gemini-2.0-flash-exp:free',        # $0.00
    'balanced': 'google/gemini-2.0-flash-lite-001',     # ~$0.00008/page
    'premium': 'google/gemini-2.5-pro',                 # ~$0.0013/page
    'claude': 'anthropic/claude-3.5-sonnet',            # ~$0.003/page (best for Thai)
    'gpt4o': 'openai/gpt-4o',                           # ~$0.0025/page
}
```

**Extraction Process:**
```python
def extract_from_pdf(pdf_path, dpi=200):
    # 1. Convert PDF pages to images
    images = convert_from_path(pdf_path, dpi=dpi)
    
    pages = []
    for i, img in enumerate(images):
        # 2. Convert image to base64
        img_b64 = base64.b64encode(img_buffer).decode()
        
        # 3. Call OpenRouter API
        response = requests.post(
            url="https://openrouter.ai/api/v1/chat/completions",
            json={
                "model": self.model_id,
                "messages": [{
                    "role": "user",
                    "content": [
                        {"type": "text", "text": PROMPT},
                        {"type": "image_url", "image_url": {"url": f"data:image/png;base64,{img_b64}"}}
                    ]
                }]
            }
        )
        
        # 4. Extract text from response
        text = response.json()['choices'][0]['message']['content']
        pages.append(text.strip())
    
    return pages
```

**Error Handling:**
```python
try:
    response.raise_for_status()
except requests.exceptions.HTTPError as e:
    if e.response.status_code == 429:
        # Rate limited - log warning and continue
        logger.warning(f"âš ï¸  Page {i}: Rate limited (429). Continuing...")
        # Don't add error placeholder, just skip
    else:
        # Other error - add error message
        pages.append(f"[Error: {str(e)}]")
```

---

### 4. **QualityChecker** (`quality_checker.py`)

**à¸«à¸™à¹‰à¸²à¸—à¸µà¹ˆ:** à¸„à¸³à¸™à¸§à¸“à¸„à¸¸à¸“à¸ à¸²à¸à¸‚à¸­à¸‡à¸‚à¹‰à¸­à¸„à¸§à¸²à¸¡à¸—à¸µà¹ˆà¸”à¸¶à¸‡à¹„à¸”à¹‰ (5 à¸¡à¸´à¸•à¸´)

**Quality Dimensions:**
```python
WEIGHTS = {
    'text_quality': 0.25,      # à¸„à¸§à¸²à¸¡à¸ªà¸°à¸­à¸²à¸” (à¹„à¸¡à¹ˆà¸¡à¸µ GLYPH, special chars)
    'word_quality': 0.20,      # à¸„à¸³à¸ªà¸¡à¸šà¸¹à¸£à¸“à¹Œ (à¹„à¸¡à¹ˆà¸¡à¸µà¸„à¸³à¸•à¸±à¸”)
    'consistency': 0.15,       # à¸„à¸§à¸²à¸¡à¸ªà¸¡à¹ˆà¸³à¹€à¸ªà¸¡à¸­à¸£à¸°à¸«à¸§à¹ˆà¸²à¸‡à¸«à¸™à¹‰à¸²
    'structure_quality': 0.20, # à¹‚à¸„à¸£à¸‡à¸ªà¸£à¹‰à¸²à¸‡ (headers, lists, tables)
    'content_density': 0.20    # à¸„à¸§à¸²à¸¡à¸«à¸™à¸²à¹à¸™à¹ˆà¸™à¸‚à¸­à¸‡à¹€à¸™à¸·à¹‰à¸­à¸«à¸²
}
```

**Scoring Algorithm:**
```python
def check_quality(pages: List[str]) -> QualityReport:
    # 1. Text Quality (0.25)
    text_quality = 1.0 - (glyph_count / max_glyphs)
    
    # 2. Word Quality (0.20)
    word_quality = 1.0 - (broken_words / total_words)
    
    # 3. Consistency (0.15)
    consistency = 1.0 - (stddev_page_length / mean_page_length)
    
    # 4. Structure Quality (0.20)
    structure_quality = (
        0.4 * header_score +
        0.3 * list_score +
        0.3 * table_score
    )
    
    # 5. Content Density (0.20)
    content_density = min(1.0, avg_words_per_page / 100)
    
    # Overall Score
    overall_score = (
        text_quality * 0.25 +
        word_quality * 0.20 +
        consistency * 0.15 +
        structure_quality * 0.20 +
        content_density * 0.20
    )
    
    return QualityReport(
        overall_score=overall_score,
        text_quality=text_quality,
        word_quality=word_quality,
        consistency=consistency,
        structure_quality=structure_quality,
        content_density=content_density
    )
```

**Quality Thresholds:**
```python
QUALITY_LEVELS = {
    0.95: 'EXCELLENT',  # à¸¢à¸­à¸”à¹€à¸¢à¸µà¹ˆà¸¢à¸¡ - à¹ƒà¸Šà¹‰à¸‡à¸²à¸™à¹„à¸”à¹‰à¸—à¸±à¸™à¸—à¸µ
    0.85: 'GOOD',       # à¸”à¸µ - à¹ƒà¸Šà¹‰à¸‡à¸²à¸™à¹„à¸”à¹‰
    0.70: 'ACCEPTABLE', # à¸à¸­à¹ƒà¸Šà¹‰ - à¸­à¸²à¸ˆà¸•à¹‰à¸­à¸‡à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸š
    0.50: 'POOR',       # à¹à¸¢à¹ˆ - à¸„à¸§à¸£ escalate
    0.00: 'FAILED'      # à¸¥à¹‰à¸¡à¹€à¸«à¸¥à¸§ - à¸•à¹‰à¸­à¸‡ escalate
}
```

---

### 5. **TextCleaner** (`text_cleaner.py`)

**à¸«à¸™à¹‰à¸²à¸—à¸µà¹ˆ:** à¸—à¸³à¸„à¸§à¸²à¸¡à¸ªà¸°à¸­à¸²à¸”à¸‚à¹‰à¸­à¸„à¸§à¸²à¸¡ à¸¥à¸š artifacts

**Cleaning Steps:**
```python
def clean_text(text: str) -> str:
    # 1. Remove GLYPH characters
    text = re.sub(r'GLYPH<[^>]+>', '', text)
    
    # 2. Remove base64 encoded images
    text = re.sub(r'!\[Image\]\(data:image/[^)]+\)', '', text)
    
    # 3. Remove HTML comments
    text = re.sub(r'<!--.*?-->', '', text, flags=re.DOTALL)
    
    # 4. Remove OCR noise (control characters)
    text = re.sub(r'[\x00-\x08\x0B\x0C\x0E-\x1F\x7F]', '', text)
    
    # 5. Fix broken Thai words (à¸ à¸²à¸£ â†’ à¸à¸²à¸£)
    text = fix_broken_words(text)
    
    # 6. Normalize Thai text (Unicode normalization)
    text = normalize_thai_text(text)
    
    # 7. Clean markdown artifacts
    text = clean_markdown_artifacts(text)
    
    return text.strip()
```

**Markdown Cleaning:**
```python
def clean_markdown_artifacts(text: str) -> str:
    # Remove HTML comments
    text = re.sub(r'<!--.*?-->', '', text, flags=re.DOTALL)
    
    # Remove broken table rows
    lines = text.split('\n')
    cleaned_lines = []
    for line in lines:
        stripped = line.strip()
        # Skip empty table rows: |, | |, ||, |||
        if stripped in ['|', '| |', '||', '|||']:
            continue
        # Skip lines with only pipes and spaces
        if stripped and all(c in '| ' for c in stripped) and len(stripped) < 5:
            continue
        cleaned_lines.append(line)
    
    text = '\n'.join(cleaned_lines)
    
    # Collapse excessive newlines (3+ â†’ 2)
    text = re.sub(r'\n{3,}', '\n\n', text)
    
    return text.strip()
```

---

## ğŸ” Validation & Quality Control

### **Fallback Detection Logic**

```python
# 1. Check if Docling extraction is valid
def should_fallback_to_full_markdown(sections):
    total_chars = sum(len(s) for s in sections)
    avg_chars = total_chars / len(sections) if sections else 0
    
    # Check for problematic content
    has_only_comments = all(
        s.strip().startswith('<!--') and 'ğŸ–¼ï¸âŒ' in s 
        for s in sections if s.strip()
    )
    
    has_base64_images = any(
        'data:image/' in s and 'base64,' in s
        for s in sections
    )
    
    # Fallback conditions
    return (
        not sections                                    # No sections
        or all(len(s.strip()) < 50 for s in sections) # All too short
        or (len(sections) < 3 and avg_chars < 500)     # Too few + short
        or has_only_comments                            # Only HTML comments
        or has_base64_images                            # Has base64 images
    )

# 2. Check if PDF is image-based (no text layer)
def is_image_based_pdf(full_markdown):
    # Remove HTML comments
    clean_text = re.sub(r'<!--.*?-->', '', full_markdown, flags=re.DOTALL).strip()
    
    # If almost nothing left, it's image-based
    return len(clean_text) < 50
```

### **Quality Escalation Logic**

```python
def should_escalate(current_tier, quality_score, target_quality):
    # Check if quality meets target
    if quality_score >= target_quality:
        return False  # No need to escalate
    
    # Check if we're at the last tier
    if current_tier == 'premium':
        return False  # Can't escalate further
    
    # Check tier-specific thresholds
    tier_threshold = TIER_THRESHOLDS[current_tier]
    if quality_score >= tier_threshold:
        return False  # Meets tier threshold
    
    return True  # Escalate to next tier
```

---

## ğŸ“Š Performance Metrics

### **Extraction Time by Tier**

| Tier | PDF Type | Pages | Time | Cost |
|------|----------|-------|------|------|
| **Fast** | Text-based | 6 | ~9s | $0.00 |
| **Balanced** | Image-based | 6 | ~60s | $0.00 |
| **Premium** | Image-based | 6 | ~360s | $0.0078 |

### **Quality Score Distribution**

```
Fast (Docling):
â”œâ”€ Text PDFs:    0.75-0.85 âœ…
â””â”€ Image PDFs:   0.00 âŒ (fallback to VLM)

Balanced (Flash):
â”œâ”€ Simple docs:  0.80-0.85 âœ…
â””â”€ Complex docs: 0.75-0.80 âš ï¸

Premium (Pro):
â”œâ”€ All docs:     0.90-0.97 âœ…
â””â”€ Thai docs:    0.85-0.95 âœ…

Claude 3.5:
â””â”€ Thai docs:    0.95-0.98 ğŸ† (best)
```

---

## ğŸ¯ Best Practices

### **1. Tier Selection Strategy**

```python
# For most documents (cost-effective)
RECOMMENDED_CONFIG = {
    'start_tier': 'fast',
    'target_quality': 0.70,
    'enable_balanced': True,   # Free VLM fallback
    'enable_premium': False,   # Only if you have credits
    'auto_retry': True
}

# For important documents (quality-first)
QUALITY_FIRST_CONFIG = {
    'start_tier': 'fast',
    'target_quality': 0.85,
    'enable_balanced': True,
    'enable_premium': True,    # Enable paid tier
    'auto_retry': True
}

# For Thai documents (best quality)
THAI_DOCUMENT_CONFIG = {
    'start_tier': 'fast',
    'target_quality': 0.90,
    'enable_balanced': False,  # Skip free tier
    'enable_premium': True,    # Use Claude 3.5
    'model_override': 'claude',
    'auto_retry': True
}
```

### **2. Cost Optimization**

```python
# Multi-model cascade (90% free, 10% paid)
def smart_extraction(pdf_path):
    # Try free tier first
    result = extract_with_free()
    if result.quality >= 0.85:
        return result  # Good enough!
    
    # For Thai docs, use Claude (best quality)
    if is_thai_document(pdf_path):
        result = extract_with_claude()
        if result.quality >= 0.90:
            return result
    
    # Last resort: Gemini Pro
    return extract_with_premium()
```

### **3. Error Handling**

```python
# Handle rate limits gracefully
def handle_rate_limit(tier):
    if tier == 'balanced':
        logger.warning("âš ï¸  Balanced tier rate limited (429)")
        logger.info("ğŸ’¡ Trying premium tier with credits...")
        return 'premium'
    elif tier == 'premium':
        logger.error("âŒ Premium tier failed")
        logger.info("ğŸ’¡ Returning best available result...")
        return None  # Use best result so far
```

---

## ğŸ› Common Issues & Solutions

### **Issue 1: "Clean markdown: 671 â†’ 0 chars"**

**Cause:** TextCleaner removing all content because it's HTML comments or base64 images

**Solution:**
```python
# Fixed in document_processor.py
has_base64_images = any('data:image/' in s for s in sections)
if has_base64_images:
    # Fallback to full markdown
    # Then clean base64 images
    text = re.sub(r'!\[Image\]\(data:image/[^)]+\)', '', text)
```

### **Issue 2: "This PDF is image-based (no text layer)"**

**Cause:** PDF is scanned image, Docling can't extract text

**Solution:**
```python
# System automatically escalates to VLM
logger.error("ğŸ’¡ This PDF is image-based. Use VLM extraction.")
# Will escalate to balanced â†’ premium
```

### **Issue 3: "Rate limited (429)"**

**Cause:** Exceeded OpenRouter/Gemini free tier limit (1500/day)

**Solutions:**
1. Wait until next day (limit resets at 00:00 UTC)
2. Add credits to OpenRouter account
3. Use premium tier (paid)
4. Use alternative API key

### **Issue 4: Slow extraction (6+ minutes)**

**Cause:** Using Gemini Pro for large PDFs

**Solutions:**
```python
# 1. Reduce DPI (faster, slightly lower quality)
image_dpi=150  # Instead of 200

# 2. Use faster model
model='claude'  # 2x faster than Gemini Pro

# 3. Lower target quality
target_quality=0.80  # Instead of 0.90
```

---

## ğŸš€ Quick Start

### **1. Basic Usage**

```python
from src.core.progressive_processor import ProgressiveDocumentProcessor

# Initialize
processor = ProgressiveDocumentProcessor(
    openrouter_api_key="your-api-key",
    enable_fast_tier=True,
    enable_balanced_tier=True,
    enable_premium_tier=False,  # Set True if you have credits
    image_dpi=200,
    disable_ocr=True
)

# Extract
result = processor.extract_with_smart_routing(
    pdf_path="document.pdf",
    target_quality=0.70,
    start_tier='fast',
    auto_retry=True,
    clean_text=True
)

# Check results
print(f"Tier used: {result.tier_used}")
print(f"Quality: {result.quality_score:.3f}")
print(f"Cost: ${result.cost:.4f}")
print(f"Pages: {len(result.pages)}")
```

### **2. Web Interface**

```bash
# Start Flask server
cd /Users/pond500/RAG/mcp_rag_v2
FLASK_APP=web/app.py python -m flask run --port 5001 --debug

# Open browser
open http://localhost:5001
```

### **3. Configuration**

```python
# Environment variables
export OPENROUTER_API_KEY="sk-or-v1-..."
export QDRANT_URL="http://localhost:6333"

# Settings (src/config/settings.py)
class Settings:
    openrouter_api_key: str = "sk-or-v1-..."
    enable_progressive: bool = True
    default_target_quality: float = 0.70
    default_image_dpi: int = 200
```

---

## ğŸ“ˆ Future Improvements

### **1. Parallel Processing**
```python
# Process multiple pages simultaneously
async def extract_parallel(pages):
    tasks = [extract_page(page) for page in pages]
    results = await asyncio.gather(*tasks)
    return results
```

### **2. Caching**
```python
# Cache extraction results
@lru_cache(maxsize=100)
def extract_cached(pdf_hash):
    if pdf_hash in cache:
        return cache[pdf_hash]
    result = extract(pdf_path)
    cache[pdf_hash] = result
    return result
```

### **3. Smart Model Selection**
```python
# Auto-select best model based on document type
def select_model(pdf_path):
    if is_thai_document(pdf_path):
        return 'claude'  # Best for Thai
    elif is_table_heavy(pdf_path):
        return 'gpt4o'   # Best for tables
    else:
        return 'free'    # Default to free
```

### **4. Quality Prediction**
```python
# Predict quality before extraction
def predict_quality(pdf_path):
    features = extract_features(pdf_path)
    predicted_quality = ml_model.predict(features)
    recommended_tier = get_tier_for_quality(predicted_quality)
    return recommended_tier
```

---

## ğŸ“š References

- **Docling**: https://github.com/DS4SD/docling
- **OpenRouter**: https://openrouter.ai/docs
- **Gemini API**: https://ai.google.dev/docs
- **Claude API**: https://docs.anthropic.com/

---

## ğŸ† Summary

à¸£à¸°à¸šà¸š Document Processor à¸—à¸µà¹ˆà¸ªà¸¡à¸šà¸¹à¸£à¸“à¹Œ:

âœ… **3-Tier Progressive Extraction** - à¹€à¸£à¸´à¹ˆà¸¡à¸–à¸¹à¸ escalate à¸•à¸²à¸¡à¸„à¸¸à¸“à¸ à¸²à¸  
âœ… **No Tesseract OCR** - à¸«à¸¥à¸µà¸à¹€à¸¥à¸µà¹ˆà¸¢à¸‡ OSD errors  
âœ… **Quality-based Routing** - 5 dimensions quality scoring  
âœ… **Cost Optimized** - 90% à¹ƒà¸Šà¹‰ free tier  
âœ… **Error Resilient** - Graceful fallback & rate limit handling  
âœ… **Multi-format Support** - PDF, DOCX, XLSX, PPTX  
âœ… **Thai Language Optimized** - Claude 3.5 for best results  

**à¸„à¹ˆà¸²à¹ƒà¸Šà¹‰à¸ˆà¹ˆà¸²à¸¢à¹€à¸‰à¸¥à¸µà¹ˆà¸¢:** $0.0003/page (à¸›à¸£à¸°à¸«à¸¢à¸±à¸” 80%)  
**à¸„à¸¸à¸“à¸ à¸²à¸à¹€à¸‰à¸¥à¸µà¹ˆà¸¢:** 0.85-0.90 (à¸”à¸µà¸¡à¸²à¸)  
**à¹€à¸§à¸¥à¸²à¹€à¸‰à¸¥à¸µà¹ˆà¸¢:** 10-60 à¸§à¸´à¸™à¸²à¸—à¸µ/à¹€à¸­à¸à¸ªà¸²à¸£  

---

*Last Updated: 2025-12-22*
