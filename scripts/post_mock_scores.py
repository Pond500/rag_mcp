#!/usr/bin/env python3
"""
Post Mock Evaluation Scores to Langfuse
‡∏™‡∏Ñ‡∏£‡∏¥‡∏õ‡∏ï‡πå‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö POST ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô mockup ‡πÑ‡∏õ‡∏¢‡∏±‡∏á Langfuse ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏£‡∏∞‡∏ö‡∏ö evaluation
‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏±‡∏ô evaluation ‡∏à‡∏£‡∏¥‡∏á
"""

import sys
import os
from pathlib import Path

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from langfuse import Langfuse
from src.observability.langfuse_config import get_langfuse_config, print_connection_info
import random
from datetime import datetime
from typing import Dict, List


def create_mock_trace(langfuse: Langfuse, trace_name: str) -> str:
    """‡∏™‡∏£‡πâ‡∏≤‡∏á mock trace ‡∏û‡∏£‡πâ‡∏≠‡∏° generation ‡πÅ‡∏•‡∏∞‡∏™‡πà‡∏á‡∏Å‡∏•‡∏±‡∏ö trace_id"""
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á generation (observation) - Langfuse ‡∏à‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á trace_id ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥
    generation = langfuse.start_observation(
        as_type="generation",
        name=trace_name,
        input={
            "query": "What is the capital of Thailand?",
            "context": ["Bangkok is the capital and most populous city of Thailand."]
        },
        metadata={
            "test": True,
            "trace_type": "mock",
            "created_at": datetime.now().isoformat()
        }
    )
    
    # ‡∏î‡∏∂‡∏á trace_id ‡∏à‡∏≤‡∏Å generation
    trace_id = generation.trace_id
    
    print(f"üìù Created mock trace: {trace_name}")
    print(f"   Trace ID: {trace_id}")
    
    # Update generation ‡∏î‡πâ‡∏ß‡∏¢ output ‡∏Å‡πà‡∏≠‡∏ô‡∏à‡∏∞ end
    generation.update(
        output={
            "answer": "Bangkok is the capital of Thailand.",
            "confidence": round(random.uniform(0.8, 1.0), 3)
        },
        metadata={
            "model": "mock-model",
            "tokens_used": random.randint(50, 200)
        }
    )
    
    # End generation
    generation.end()
    
    return trace_id


def post_mock_rag_scores(langfuse: Langfuse, trace_id: str, trace_name: str):
    """POST ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô RAG evaluation ‡πÅ‡∏ö‡∏ö mockup
    
    ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏ó‡∏µ‡πà POST:
    - faithfulness (0-1): ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏≤‡∏° context
    - answer_relevancy (0-1): ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö
    - context_precision (0-1): ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏Ç‡∏≠‡∏á context ‡∏ó‡∏µ‡πà retrieve
    - context_recall (0-1): ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô‡∏Ç‡∏≠‡∏á context
    """
    
    scores = {
        "faithfulness": round(random.uniform(0.7, 1.0), 3),
        "answer_relevancy": round(random.uniform(0.6, 0.95), 3),
        "context_precision": round(random.uniform(0.65, 0.9), 3),
        "context_recall": round(random.uniform(0.7, 0.95), 3),
        "correctness": round(random.uniform(0.7, 0.95), 3),
        "helpfulness": round(random.uniform(0.7, 0.95), 3),
        "harmfulness": round(random.uniform(0.7, 0.95), 3),
        "semantic_similarity": round(random.uniform(0.7, 0.95), 3)
        
    }
    
    print(f"\nüìä Posting mock scores for trace: {trace_name}")
    print(f"   Trace ID: {trace_id}")
    
    for metric_name, score_value in scores.items():
        langfuse.create_score(
            trace_id=trace_id,
            name=metric_name,
            value=score_value,
            comment=f"Mock score generated for testing (not real evaluation)"
        )
        print(f"   ‚úÖ {metric_name}: {score_value:.3f}")


def post_mock_llm_scores(langfuse: Langfuse, trace_id: str, trace_name: str):
    """POST ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô LLM quality ‡πÅ‡∏ö‡∏ö mockup
    
    ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏ó‡∏µ‡πà POST:
    - hallucination_score (0-1): ‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Å‡∏≤‡∏£ hallucinate (‡∏ï‡πà‡∏≥ = ‡∏î‡∏µ)
    - toxicity_score (0-1): ‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡∏û‡∏¥‡∏© (‡∏ï‡πà‡∏≥ = ‡∏î‡∏µ)
    - coherence (0-1): ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≠‡∏î‡∏Ñ‡∏•‡πâ‡∏≠‡∏á‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö (‡∏™‡∏π‡∏á = ‡∏î‡∏µ)
    - fluency (0-1): ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏•‡∏∑‡πà‡∏ô‡πÑ‡∏´‡∏•‡∏Ç‡∏≠‡∏á‡∏†‡∏≤‡∏©‡∏≤ (‡∏™‡∏π‡∏á = ‡∏î‡∏µ)
    """
    
    scores = {
        "hallucination_score": round(random.uniform(0.0, 0.3), 3),  # ‡∏ï‡πà‡∏≥ = ‡∏î‡∏µ
        "toxicity_score": round(random.uniform(0.0, 0.2), 3),       # ‡∏ï‡πà‡∏≥ = ‡∏î‡∏µ
        "coherence": round(random.uniform(0.75, 1.0), 3),           # ‡∏™‡∏π‡∏á = ‡∏î‡∏µ
        "fluency": round(random.uniform(0.8, 1.0), 3),              # ‡∏™‡∏π‡∏á = ‡∏î‡∏µ
    }
    
    print(f"\nüìä Posting mock LLM quality scores for trace: {trace_name}")
    print(f"   Trace ID: {trace_id}")
    
    for metric_name, score_value in scores.items():
        langfuse.create_score(
            trace_id=trace_id,
            name=metric_name,
            value=score_value,
            comment=f"Mock score generated for testing (not real evaluation)"
        )
        print(f"   ‚úÖ {metric_name}: {score_value:.3f}")


def post_mock_user_feedback(langfuse: Langfuse, trace_id: str, trace_name: str):
    """POST ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô user feedback ‡πÅ‡∏ö‡∏ö mockup
    
    ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏ó‡∏µ‡πà POST:
    - user_rating (1-5): ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏à‡∏≤‡∏Å‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ
    - thumbs_up (0/1): ‡∏ñ‡∏π‡∏Å‡πÉ‡∏à‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
    """
    
    user_rating = random.randint(3, 5)
    thumbs_up = 1 if user_rating >= 4 else 0
    
    print(f"\nüë§ Posting mock user feedback for trace: {trace_name}")
    print(f"   Trace ID: {trace_id}")
    
    langfuse.create_score(
        trace_id=trace_id,
        name="user_rating",
        value=user_rating,
        comment=f"Mock user rating for testing"
    )
    print(f"   ‚≠ê user_rating: {user_rating}/5")
    
    langfuse.create_score(
        trace_id=trace_id,
        name="thumbs_up",
        value=thumbs_up,
        comment=f"Mock thumbs up for testing"
    )
    print(f"   üëç thumbs_up: {'Yes' if thumbs_up else 'No'}")


def post_mock_custom_scores(langfuse: Langfuse, trace_id: str, trace_name: str, 
                            custom_metrics: Dict[str, float]):
    """POST ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô custom metrics ‡πÅ‡∏ö‡∏ö mockup
    
    Args:
        custom_metrics: Dict ‡∏Ç‡∏≠‡∏á metric_name: score_value
    """
    
    print(f"\nüîß Posting mock custom scores for trace: {trace_name}")
    print(f"   Trace ID: {trace_id}")
    
    for metric_name, score_value in custom_metrics.items():
        langfuse.create_score(
            trace_id=trace_id,
            name=metric_name,
            value=score_value,
            comment=f"Mock custom score for testing"
        )
        print(f"   ‚úÖ {metric_name}: {score_value}")


def main():
    """Main function"""
    print("=" * 70)
    print("üìù POST MOCK EVALUATION SCORES TO LANGFUSE")
    print("=" * 70)
    
    # Load config
    print_connection_info()
    config = get_langfuse_config()
    
    # Validate config
    valid, message = config.validate()
    if not valid:
        print(f"\n{message}")
        print("‚ùå ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Langfuse environment variables ‡∏Å‡πà‡∏≠‡∏ô")
        sys.exit(1)
    
    if not config.enabled:
        print("\n‚ö†Ô∏è  Langfuse ‡∏ñ‡∏π‡∏Å disable (LANGFUSE_ENABLED=false)")
        print("üí° ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ LANGFUSE_ENABLED=true ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô")
        sys.exit(1)
    
    # Initialize Langfuse
    print(f"\nüîó ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏Å‡∏±‡∏ö Langfuse: {config.host}")
    langfuse = Langfuse(
        public_key=config.public_key,
        secret_key=config.secret_key,
        host=config.host,
        debug=config.debug
    )
    
    # Create mock traces and post scores
    print("\n" + "=" * 70)
    print("üöÄ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏™‡∏£‡πâ‡∏≤‡∏á mock traces ‡πÅ‡∏•‡∏∞ POST ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô (10 ‡∏£‡∏≠‡∏ö)")
    print("=" * 70)
    
    # ‡∏™‡∏∏‡πà‡∏°‡∏™‡∏£‡πâ‡∏≤‡∏á mock traces 10 ‡∏£‡∏≠‡∏ö (‡∏ó‡∏∏‡∏Å trace ‡πÑ‡∏î‡πâ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏Ñ‡∏£‡∏ö‡∏ó‡∏∏‡∏Å metric)
    for i in range(1, 11):
        print(f"\n{'='*70}")
        print(f"üîÑ ‡∏£‡∏≠‡∏ö‡∏ó‡∏µ‡πà {i}/10")
        print(f"{'='*70}")
        
        # ‡∏ó‡∏∏‡∏Å trace ‡πÉ‡∏ä‡πâ‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô: "evaluation:ragas"
        trace_name = "evaluation:ragas"
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á trace
        trace_id = create_mock_trace(langfuse, trace_name)
        
        # POST ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏Ñ‡∏£‡∏ö‡∏ó‡∏∏‡∏Å metric
        # 1. RAG Metrics
        post_mock_rag_scores(langfuse, trace_id, trace_name)
        
        # 2. LLM Quality Metrics
        post_mock_llm_scores(langfuse, trace_id, trace_name)
        
        # 3. User Feedback
        post_mock_user_feedback(langfuse, trace_id, trace_name)
        
        # 4. Custom Performance Metrics
        post_mock_custom_scores(
            langfuse, 
            trace_id, 
            trace_name,
            {
                "response_time_ms": round(random.uniform(200, 800), 2),
                "retrieval_count": random.randint(3, 10),
                "chunk_relevance_avg": round(random.uniform(0.6, 0.95), 3),
                "reranker_score": round(random.uniform(0.7, 0.98), 3)
            }
        )
    
    # Flush to ensure all data is sent
    print("\nüì§ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏õ‡∏¢‡∏±‡∏á Langfuse...")
    langfuse.flush()
    
    print("\n" + "=" * 70)
    print("‚úÖ POST MOCK SCORES ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô")
    print("=" * 70)
    print(f"\nüåê ‡∏î‡∏π‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÑ‡∏î‡πâ‡∏ó‡∏µ‡πà: {config.host}")
    print("üìä ‡πÑ‡∏õ‡∏ó‡∏µ‡πà Dashboard ‚Üí Traces ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏π mock traces ‡∏ó‡∏µ‡πà‡∏™‡∏£‡πâ‡∏≤‡∏á")
    print("üìà ‡πÑ‡∏õ‡∏ó‡∏µ‡πà Scores/Evaluations ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏π‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏ó‡∏µ‡πà POST")
    print("\nüí° TIP: ‡∏Ñ‡∏∏‡∏ì‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ customize metrics ‡πÑ‡∏î‡πâ‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç")
    print("   ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô post_mock_custom_scores() ‡πÉ‡∏ô‡∏™‡∏Ñ‡∏£‡∏¥‡∏õ‡∏ï‡πå‡∏ô‡∏µ‡πâ")


if __name__ == "__main__":
    main()
